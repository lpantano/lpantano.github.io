<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>hpc on My bits</title>
    <link>http://lpantano.github.io/tags/hpc/</link>
    <description>Recent content in hpc on My bits</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 12 Jul 2019 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="http://lpantano.github.io/tags/hpc/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>How to set up cellranger to make your hpc admin happy</title>
      <link>http://lpantano.github.io/post/2019/2019-07-12-cellranger-efficiency-in-hpc/</link>
      <pubDate>Fri, 12 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>http://lpantano.github.io/post/2019/2019-07-12-cellranger-efficiency-in-hpc/</guid>
      <description>

&lt;p&gt;I found myself to force to use &lt;a href=&#34;https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/advanced/cluster-mode&#34; target=&#34;_blank&#34;&gt;cellranger&lt;/a&gt;. Meanwhile it helps a lot to
run from &lt;a href=&#34;https://www.illumina.com/informatics/sequencing-data-analysis/sequence-file-formats.html&#34; target=&#34;_blank&#34;&gt;bcl&lt;/a&gt; files to single cell counts matrixes, I discovered that is
quite difficult to control many options related to optimization.&lt;/p&gt;

&lt;p&gt;I have to run more than 200 samples in a short time of period. In my current
position at MIT, I joined the OpenMind cluster in the McGovern institute. I was
pleased to find a very flexible cluster, but as any other cluster you need to
respect other users.&lt;/p&gt;

&lt;p&gt;My samples are human samples, so I need a lot of memory so &lt;a href=&#34;https://github.com/alexdobin/STAR&#34; target=&#34;_blank&#34;&gt;STAR&lt;/a&gt; can run. The
command line is easy to set up:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;cellranger count --id=SAMPLE --transcriptome=GRCh38_pre_mRNA --fastqs=PATH2FASTQS --sample=SAMPLE&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Of course, that is in local mode. If you want to set up a job to a cluster,
it would look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash
#SBATCH -N 1
#SBATCH -c 6
#SBATCH --mem=12000
#SBATCH -t 0-48:00:00
#SBATCH -J cellr
#SBATCH -e job-counts.e
#SBATCH -o job-counts.o
## SBATCH --mail-type=END,FAIL # this line is commented
## SBATCH --mail-user=user@mit.edu  # this line is commented

cellranger count --id=SAMPLE --transcriptome=GRCh38_pre_mRNA \
          --fastqs=PATH2FASTQS --sample=SAMPLE \
          --localcores=6 --localmem=120

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The main problem with that is you are using 120G during all the process that could
go to 1-2 days.&lt;/p&gt;

&lt;p&gt;I discovered that you can setup the &lt;code&gt;cluster&lt;/code&gt; mode to make &lt;code&gt;cellranger&lt;/code&gt; to send
jobs to the cluster. However, &lt;code&gt;slurm&lt;/code&gt; is not a option to the documentation, BUT
you can specify a template file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash
#SBATCH -J __MRO_JOB_NAME__
#SBATCH -N 1
#SBATCH -c __MRO_THREADS__
#SBATCH --mem=__MRO_MEM_GB__G
#SBATCH -o __MRO_STDOUT__
#SBATCH -e __MRO_STDERR__
#SBATCH -t 0-48:00:00 
#SBATCH --qos=&amp;quot;normal&amp;quot;

 __MRO_CMD__
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The command would look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cellranger count --id=SAMPLE --transcriptome=GRCh38_pre_mRNA \ 
          --fastqs=PATH2FASTQS --sample=SAMPLE \
          --jobmode=slurm.template \
          --maxjobs=3 --jobinterval=1000

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I was feeling victory, ha ha ha, BUT I saw that the &lt;code&gt;ALIGN&lt;/code&gt; step was requesting
220G of memory&amp;hellip;. WHAT!!!!!????? or better &lt;a href=&#34;https://www.destroyallsoftware.com/talks/wat&#34; target=&#34;_blank&#34;&gt;WATTT????!!!!!&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I guess I didn&amp;rsquo;t look at the first place I should have looked, but it took me
some time to discovered that the memory is coming from a file in the reference
genome folder:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat GRCh38_pre_mRNA/reference.json
{
    &amp;quot;fasta_hash&amp;quot;: &amp;quot;954a9c3916ef1544c9358445440b9683fc061c71&amp;quot;,
    &amp;quot;genomes&amp;quot;: [
        &amp;quot;GRCh38_pre_mRNA&amp;quot;
    ],
    &amp;quot;gtf_hash&amp;quot;: &amp;quot;12aae438159b35282adaf643cca4dc5887b0448a&amp;quot;,
    &amp;quot;input_fasta_files&amp;quot;: [
        &amp;quot;Homo_sapiens.GRCh38.dna.toplevel.fa&amp;quot;
    ],
    &amp;quot;input_gtf_files&amp;quot;: [
        &amp;quot;GRC38_preRNA.gtf&amp;quot;
    ],
    &amp;quot;mem_gb&amp;quot;: 220,
    &amp;quot;mkref_version&amp;quot;: &amp;quot;3.0.2&amp;quot;,
    &amp;quot;threads&amp;quot;: 24,
    &amp;quot;version&amp;quot;: null
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I realized that when I found, after 100 &lt;code&gt;ls/cat/less/grep&lt;/code&gt;, this file with these
lines of code:&lt;/p&gt;

&lt;p&gt;````
cellranger-3.0.2/cellranger-cs/3.0.2/lib/python/cellranger/utils.py
def _load_reference_metadata_file(reference_path):
    reference_metadata_file = os.path.join(reference_path, cr_constants.REFERENCE_METADATA_FILE)
    with open(reference_metadata_file, &amp;lsquo;r&amp;rsquo;) as f:
        return json.load(f)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
So, easy peasy, I changed that to 110. I relaunched the job I was using to test,
and found that it was still setting up 220G...WHY!?!!? well, it seems if you relaunch 
in a step where the alignment has already begun, there is a cache for the resources.
So after more time trying to figure out this and asking myself, why I decided my
career path to be bioinformatics, I tried a fresh start and...IT WORKED. 

&amp;gt; How happy one can be to see a job requesting the expected resources.

&amp;gt; Incredible happy!

I went a step further, and looked how much memory actually it was using during the
alignment step, and I saw that actually only needs 64G, so I updated to 72G, to 
gain more karma from the HPC world.

There is one more thing, that will set jobs of 72G and 4 cores, if you want to 
increase the number of cores, then use this option `--mempercore 6` or whatever number
that you want: `mempercore = 72G/final_cores`.

## Workflow

Finally, I felt that I was doing my best to behave and avoid making angry other
people using the hpc. The final setup looks like this:


* meta-script to send all the samples to a master job: `runner.sh`. It accepts
      a file where first column is `sample name` and second column is `fastq_path`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;set -o pipefail  # trace ERR through pipes
set -o errtrace  # trace ERR through &amp;lsquo;time command&amp;rsquo; and other functions
set -o nounset   ## set -u : exit the script if you try to use an uninitialised variable
set -o errexit   ## set -e : exit the script if any statement returns a non-true return value&lt;/p&gt;

&lt;h1 id=&#34;set-v-you-can-uncomment-all-these-lines-to-have-a-better-debugging&#34;&gt;set -v # you can uncomment all these lines to have a better debugging&lt;/h1&gt;

&lt;h1 id=&#34;set-x&#34;&gt;set -x&lt;/h1&gt;

&lt;h1 id=&#34;export-ps4-bash-source-lineno-funcname-0-funcname-0&#34;&gt;export PS4=&amp;lsquo;+(${BASH_SOURCE}:${LINENO}): ${FUNCNAME[0]:+${FUNCNAME[0]}(): }&amp;rsquo;&lt;/h1&gt;

&lt;p&gt;while IFS=&amp;rdquo; &amp;ldquo; read -r sample paths&lt;/p&gt;

&lt;p&gt;do&lt;/p&gt;

&lt;p&gt;RUN=0
if [[ ! -e $sample/filtered_feature_bc_matrix.h5 ]]; then
    RUN=1 # run if the final output is not there yet
fi&lt;/p&gt;

&lt;p&gt;if [[  -e  $sample/_lock ]]; then
    echo $sample is locked, please:
    echo rm $sample/_lock
    RUN=0 # but don&amp;rsquo;t do anything if is locked
fi&lt;/p&gt;

&lt;p&gt;if [[ $RUN == 1 ]]; then
    echo sbatch -J $sample -o logs/$sample.o -e logs/$sample.e scripts/om-counts-cluster.slurm $sample $paths
    sbatch -J $sample -o logs/$sample.o -e logs/$sample.e scripts/om-counts-cluster.slurm $sample $paths
fi&lt;/p&gt;

&lt;p&gt;done &amp;lt; $1&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
* SBATCH script with cellranger command: `counter.slurm`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;#!/bin/bash
#SBATCH -N 1
#SBATCH -c 1
#SBATCH &amp;ndash;mem=6000
#SBATCH -t 0-48:00:00&lt;/p&gt;

&lt;h1 id=&#34;sbatch-j-cellr-1&#34;&gt;SBATCH -J &amp;ldquo;cellr-$1&amp;rdquo;&lt;/h1&gt;

&lt;h1 id=&#34;sbatch-e-job-counts-1-e&#34;&gt;SBATCH -e job-counts-$1.e&lt;/h1&gt;

&lt;h1 id=&#34;sbatch-o-job-counts-1-o&#34;&gt;SBATCH -o job-counts-$1.o&lt;/h1&gt;

&lt;h2 id=&#34;sbatch-mail-type-end-fail-this-line-is-commented&#34;&gt;SBATCH &amp;ndash;mail-type=END,FAIL # this line is commented&lt;/h2&gt;

&lt;h2 id=&#34;sbatch-mail-user-user-mit-edu-this-line-is-commented&#34;&gt;SBATCH &amp;ndash;mail-user=user@mit.edu  # this line is commented&lt;/h2&gt;

&lt;p&gt;SCRIPTPATH=&amp;ldquo;PATHTOSCRIPTS&amp;rdquo;
cellranger count &amp;ndash;id=$1 &amp;ndash;transcriptome=GRCh38_pre_mRNA &amp;ndash;fastqs=$2 &amp;ndash;sample=$1 &amp;ndash;jobmode=$SCRIPTPATH/slurm.template &amp;ndash;maxjobs=3 &amp;ndash;jobinterval=1000&lt;/p&gt;

&lt;p&gt;if [ $? -eq 0 ]; then
    echo OK
    mv $1 results/.
else
    echo $1 FAIL
fi
```&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;template script for &lt;code&gt;cellranger&lt;/code&gt; as shown above&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;other-tips&#34;&gt;Other tips&lt;/h3&gt;

&lt;p&gt;I used &lt;code&gt;mxjobs=3&lt;/code&gt; to avoid a lot of jobs. I am testing how things go, this could be spanned to bigger numbers.&lt;/p&gt;

&lt;p&gt;I used &lt;code&gt;jobinterval=1000&lt;/code&gt; to avoid sending jobs at the same time, just in case, but I don&amp;rsquo;t have any proof to support this.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;happiness in bioinformatics:&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
  </channel>
</rss>
